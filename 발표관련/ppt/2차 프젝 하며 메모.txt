2차 프젝 하며 메모

OCR도 2 stage 
text detection  -> Text recognition

이렇게 되는데, 1 stage로
Detection과 Recognition을 동시에 해내는 End-to-End OCR 모델도 연구 중 이라고하네요

현재 저희가 하는 OCR은
text detection :
색상점등을 낮춰서 특이점 추출 -> 단순히 object detection 기법 뿐 아니라 Segmentation 기법도 동원되며, 문자가 가지는 독특한 특성까지 고려하여 디텍트

Text recognition :
검출된 영역의 문자가 무엇인지 인식, 한개의 문자문자별로 인식하고 다시 결합 ( 아마 저희가 오브젝트 디텍션할때 라벨따라서 Large Truck , small Truck 나누듯 알파벳 개수만큼 라벨하여 파악한후 합치는것으로 파악 )




이슈 - 

처음보는 lmdb에대해 알아보고, 커스텀 데이터를 어떻게 넣어야하는가에 대해 고민

OCR관련, 처음엔 text detection을 통한 이미지로 text recognition이 과정을 몰랐어서
한번에 이루어지는 모델만을 찾아보았고 이로인해 오랜 시간소요. 
각각 다른 모델을 통해 2번으로 OCR이 이루어짐을 깨닫고 각각모델 찾기 


환경 세팅이 너무 어려웠음 3시간 걸린듯. 특히  warpctc-pytorch  설치가 되지않아서 엉청 애를먹었음
이를위해 torch 1.2.1 ( 깃에는 1.3.1을 명시했지만, 공식 torch사이트에는 1.3.1이없음 )
근데 torchvision쪽으로 오류생겨서 1.4.1 했음
등등 선생님께 물어봐서 버전도 낮추고 했으나 설치가 힘들어서 논문적용을 안햇음


_data_loader의 환경문제 
1번 이미지처럼 
Exception has occurred: TypeError
can't pickle Environment objects 
가 떴는데 이는 실행하는곳에서의 환경이 차이나기때문이였다 ( GPU가 1개여서 )
데이터 로더 생성시 num_workers = 0을 선언해줌으로써 해결하였다.
2번 이미지 참고

비슷하게 배치사이즈에서 오류생겨서 수정


AttributeError: '_SingleProcessDataLoaderIter' object has no attribute 'next'  에러관련
구글링 결과 리눅스에서는 잘되는데 윈도우의 경우 발생하는 문제로 보임



4번 너무 과한 학습 시간
줄이기 위한 노력 - 배치사이즈 증가 - 동일한 시간 , 오히려 늘어난 시간
알고보니 Git 과 환경을 동일시 하기위해 낮춘 토치의 버전으로인해 GPU인식 불가, CPU로 학습 진행이였던것 ( 디버깅으로 print(torch.cuda.is_available()) 에서 False가 나오는것을 확인 ) 
기존 사용하였던 환경을 복사하여 Git환경과 무관하게 최신 모델로 학습 진행 -> 문제없이 기동, GPU 사용 가능

5번 
배치사이즈 20,10씩 줄이며 딱맞는 크기로 찾아냄.학습최적화

6번 절반 이상의 학습이 완료되었는데, 리눅스 윈도우 차이 next 미처 수정하지 못한 부분에서 디버깅오류 발생
눈물을 머금고 해당 부분 수정하여 재 학습


text detection >
텍스트 디텍션을 사각형으로 하는것이아닌 polylines를 통해 도형 모형으로 디텍트 하는것 이였음.
즉, 이미지를 자르기 위해서는 x값의 최대와 y값의 최대, 그리고 x와y의 최소값을 찾아내기위해 모든 poly 라인들을 순회하여 이미지를 잘라야했음. 이걸 인지하지 못하여 많은 시간을 Test하느라 소모하게됨.

추가적으로 이미지 추출에 어려움을 겪은것은 이미지를 모델과 넣었을때, 이미지에 preprocessing 이 필요하다는것 이였는데,
preprocessing 코드와 link 맵을 만드는 코드가 저장 코드와 혼동이 되어 어려웟음
추가적으로 나온 결과 값에 대해서 Post-processing과 coordinate adjustment(좌표 조정) 이 필요했는데 이때문에 이미지에 제대로된 사각형을 잡는것에 구현어려움을 겪음



Text recognition
모델을 사용하기 위해 커스텀 데이터 로더 필요.
원래 DTRB 코드에서는 torch.utils.data.DataLoader를 통해 경로값으로 이미지를 받아 모델에 적용하였음.
하지만 우리가 하고자하는건 Text detection으로부터 추출된 Numpy배열이미지를 통해 모델에 적용해야함.
즉 Numpy 배열의 이미지를 Numpy를 Tensor로 변경하고 그 변경한 Tensor들을 이용하여 TensorDataset을 만들고 이를 이용하여 DataLoader

근데 이미지를 로드할때 단순 imread가 아니라 여러 전처리가 함수적으로 구현이 되어있어서 커스텀하기 불편하였음
cv2 ->(차원낮추기)-> numpy -> tensor



둘다 이슈인데,
원본 코드는 이미지 경로를 입력해서 모델이 동작하도록 모두 구현
이를 Numpy이미지를 넣을때 되도록 전처리가 필요했음.



Text Detection으로 cv2형식 넘파이를 가져왔는데
Recognition에선 PIL을 써서 가져온 형식을 다 변환해야햇음 커스텀 데이터를 짜며 고생




이미지가 세로가 된경우 영어를 인식 못하는 이슈 발생
세로길이가 가로보다 1.5배 길 경우 90도를 돌려서 확인해봄
단 여기서 오른쪽으로 90, 왼쪽으로 90 둘다 해서 점수가 높은걸 채택하도록함

비슷하게 이미지 상하 반전도 시켜서 학습해서 더 높은걸 채용

이를 구현하기 위해 아예 처음 Detection부터 이미지 넘겨주는 것 함수 자체를 변경하였음




번역 부분에 평균 색상으로 채우고자할때 -> 그냥 np.mean 함수 쓰면 쉬울줄 알았는데 차원이 3개인 RGB모두 고려해야해서 쉽지않았음.
mean으로 처리해버리면 그냥 Gray색으로만 평균값을 구해서 모두 회색으로만 나옴. ( 7번 )

블러처리할 이미지의 색상값 구하기.
처음 생각한 것 - 가장 쉽지만 가장 비효율적인 : 처음 아이디어를 따라서 단순하게 모든 차원당 평균 픽셀 값 찾는것. >> 결과가 호도하고 은근 부정확함 : 가장 주된 색상이 아닌 모든 픽셀값의 평균을 고려하기 때문에 >> 대비가 큰 이미지(한 이미지 안에 밝고 어두운 부분이 있는경우) 로는 별로임
이를 개선한 방법 - 비지도학습인 K-Means clustering 을 통해 색상군 도출해내기.
이를 적용하였으나, 현 목적에는 맞지않아 그냥 평균값으로


번역문제 en-ko 모델의 부족
한글 text문제 - pil 변환 필요


가로세로 글자 인식 ( 90, 180, 270 돌려서 가장 확률 높은것을 사용할까 싶었으나, 시간 단축을 위해 넓이가 높이의 1.5배 이상일 경우로 나눠서함 )



느리다 모델 3개라서